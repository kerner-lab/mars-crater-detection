{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KGVY9jNwRYnG"
   },
   "outputs": [],
   "source": [
    "!cp '/content/drive/MyDrive/dewarping/unet_data_edges.zip' ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vF_QzExoPb3N"
   },
   "outputs": [],
   "source": [
    "# !unzip -qq unet_data.zip  \n",
    "# !unzip -qq unet_data328.zip  \n",
    "# !unzip -qq unet_data438.zip  \n",
    "!unzip -qq unet_data_edges.zip  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "an8eZFwPPXg0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-06 12:21:15.686368: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "import tensorflow as tf\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as keras\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from keras import layers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model, load_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YvZMqWw5NSLb"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSEBKnOAX86y"
   },
   "source": [
    "**U2NET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Url28Q47BaqQ"
   },
   "outputs": [],
   "source": [
    "def loss(y_true, y_pred):\n",
    "    y_pred = tf.expand_dims(y_pred, axis=-1)\n",
    "    # loss0 = tf.nn.weighted_cross_entropy_with_logits(y_true, y_pred[0], pos_weight=2, name=None)\n",
    "    # loss1 = tf.nn.weighted_cross_entropy_with_logits(y_true, y_pred[1], pos_weight=2, name=None)\n",
    "    # loss2 = tf.nn.weighted_cross_entropy_with_logits(y_true, y_pred[2], pos_weight=2, name=None)\n",
    "    # loss3 = tf.nn.weighted_cross_entropy_with_logits(y_true, y_pred[3], pos_weight=2, name=None)\n",
    "    # loss4 = tf.nn.weighted_cross_entropy_with_logits(y_true, y_pred[4], pos_weight=2, name=None)\n",
    "    # loss5 = tf.nn.weighted_cross_entropy_with_logits(y_true, y_pred[5], pos_weight=2, name=None)\n",
    "    # loss6 = tf.nn.weighted_cross_entropy_with_logits(y_true, y_pred[6], pos_weight=2, name=None)\n",
    "\n",
    "    loss0 = bce(y_true, y_pred[0])\n",
    "    loss1 = bce(y_true, y_pred[1])\n",
    "    loss2 = bce(y_true, y_pred[2])\n",
    "    loss3 = bce(y_true, y_pred[3])\n",
    "    loss4 = bce(y_true, y_pred[4])\n",
    "    loss5 = bce(y_true, y_pred[5])\n",
    "    loss6 = bce(y_true, y_pred[6])\n",
    "    return loss0 + loss1 + loss2 + loss3 + loss4 + loss5 + loss6\n",
    "\n",
    "def REBNCONV(x, out_ch=3, dirate=1):\n",
    "    #x = ZeroPadding2D((1*dirate,1*dirate))(x)\n",
    "    x = Conv2D(out_ch, 3, padding='same', dilation_rate = 1*dirate)(x)\n",
    "    x = BatchNormalization(axis=3)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def _upsample_like(src, tar):\n",
    "    h = int(tar.shape[1]/src.shape[1])\n",
    "    w = int(tar.shape[2]/src.shape[2])\n",
    "    src = UpSampling2D((h,w), interpolation='bilinear')(src)\n",
    "    return src\n",
    "\n",
    "def RSU7(x, mid_ch=12, out_ch=3):\n",
    "    \n",
    "    x0 = REBNCONV(x, out_ch, 1)\n",
    "    \n",
    "    x1 = REBNCONV(x0, mid_ch, 1)\n",
    "    x = MaxPool2D(2, 2)(x1)\n",
    "\n",
    "    x2 = REBNCONV(x, mid_ch, 1)\n",
    "    x = MaxPool2D(2, 2)(x2)\n",
    "\n",
    "    x3 = REBNCONV(x, mid_ch, 1)\n",
    "    x = MaxPool2D(2, 2)(x3)\n",
    "\n",
    "    x4 = REBNCONV(x, mid_ch, 1)\n",
    "    x = MaxPool2D(2, 2)(x4)\n",
    "\n",
    "    x5 = REBNCONV(x, mid_ch, 1)\n",
    "    x = MaxPool2D(2, 2)(x5)\n",
    "\n",
    "    x6 = REBNCONV(x, mid_ch, 1)\n",
    "\n",
    "    x = REBNCONV(x6, mid_ch, 2)\n",
    "\n",
    "    x = REBNCONV(tf.concat([x,x6],axis=-1), mid_ch, 1)\n",
    "    x = _upsample_like(x,x5)\n",
    "\n",
    "    x = REBNCONV(tf.concat([x,x5],axis=-1), mid_ch, 1)\n",
    "    x = _upsample_like(x,x4)\n",
    "\n",
    "    x = REBNCONV(tf.concat([x,x4],axis=-1), mid_ch, 1)\n",
    "    x = _upsample_like(x,x3)\n",
    "\n",
    "    x = REBNCONV(tf.concat([x,x3],axis=-1), mid_ch, 1)\n",
    "    x = _upsample_like(x,x2)\n",
    "\n",
    "    x = REBNCONV(tf.concat([x,x2],axis=-1), mid_ch, 1)\n",
    "    x = _upsample_like(x,x1)\n",
    "\n",
    "    x = REBNCONV(tf.concat([x,x1],axis=-1), out_ch, 1)\n",
    "\n",
    "    return x + x0\n",
    "\n",
    "def RSU6(x, mid_ch=12, out_ch=3):\n",
    "    \n",
    "    x0 = REBNCONV(x, out_ch, 1)\n",
    "    \n",
    "    x1 = REBNCONV(x0, mid_ch, 1)\n",
    "    x = MaxPool2D(2, 2)(x1)\n",
    "\n",
    "    x2 = REBNCONV(x, mid_ch, 1)\n",
    "    x = MaxPool2D(2, 2)(x2)\n",
    "\n",
    "    x3 = REBNCONV(x, mid_ch, 1)\n",
    "    x = MaxPool2D(2, 2)(x3)\n",
    "\n",
    "    x4 = REBNCONV(x, mid_ch, 1)\n",
    "    x = MaxPool2D(2, 2)(x4)\n",
    "\n",
    "    x5 = REBNCONV(x, mid_ch, 1)\n",
    "\n",
    "    x = REBNCONV(x, mid_ch, 2)\n",
    "\n",
    "    x = REBNCONV(tf.concat([x,x5],axis=-1), mid_ch, 1)\n",
    "    x = _upsample_like(x,x4)\n",
    "\n",
    "    x = REBNCONV(tf.concat([x,x4],axis=-1), mid_ch, 1)\n",
    "    x = _upsample_like(x,x3)\n",
    "\n",
    "    x = REBNCONV(tf.concat([x,x3],axis=-1), mid_ch, 1)\n",
    "    x = _upsample_like(x,x2)\n",
    "\n",
    "    x = REBNCONV(tf.concat([x,x2],axis=-1), mid_ch, 1)\n",
    "    x = _upsample_like(x,x1)\n",
    "\n",
    "    x = REBNCONV(tf.concat([x,x1],axis=-1), out_ch, 1)\n",
    "\n",
    "    return x + x0\n",
    "\n",
    "def RSU5(x, mid_ch=12, out_ch=3):\n",
    "    \n",
    "    x0 = REBNCONV(x, out_ch, 1)\n",
    "    \n",
    "    x1 = REBNCONV(x0, mid_ch, 1)\n",
    "    x = MaxPool2D(2, 2)(x1)\n",
    "\n",
    "    x2 = REBNCONV(x, mid_ch, 1)\n",
    "    x = MaxPool2D(2, 2)(x2)\n",
    "\n",
    "    x3 = REBNCONV(x, mid_ch, 1)\n",
    "    x = MaxPool2D(2, 2)(x3)\n",
    "\n",
    "    x4 = REBNCONV(x, mid_ch, 1)\n",
    "\n",
    "    x = REBNCONV(x, mid_ch, 2)\n",
    "\n",
    "    x = REBNCONV(tf.concat([x,x4],axis=-1), mid_ch, 1)\n",
    "    x = _upsample_like(x,x3)\n",
    "\n",
    "    x = REBNCONV(tf.concat([x,x3],axis=-1), mid_ch, 1)\n",
    "    x = _upsample_like(x,x2)\n",
    "\n",
    "    x = REBNCONV(tf.concat([x,x2],axis=-1), mid_ch, 1)\n",
    "    x = _upsample_like(x,x1)\n",
    "\n",
    "    x = REBNCONV(tf.concat([x,x1],axis=-1), out_ch, 1)\n",
    "\n",
    "    return x + x0\n",
    "\n",
    "def RSU4(x, mid_ch=12, out_ch=3):\n",
    "    \n",
    "    x0 = REBNCONV(x, out_ch, 1)\n",
    "    \n",
    "    x1 = REBNCONV(x0, mid_ch, 1)\n",
    "    x = MaxPool2D(2, 2)(x1)\n",
    "\n",
    "    x2 = REBNCONV(x, mid_ch, 1)\n",
    "    x = MaxPool2D(2, 2)(x2)\n",
    "\n",
    "    x3 = REBNCONV(x, mid_ch, 1)\n",
    "\n",
    "    x = REBNCONV(x, mid_ch, 2)\n",
    "\n",
    "    x = REBNCONV(tf.concat([x,x3],axis=-1), mid_ch, 1)\n",
    "    x = _upsample_like(x,x2)\n",
    "\n",
    "    x = REBNCONV(tf.concat([x,x2],axis=-1), mid_ch, 1)\n",
    "    x = _upsample_like(x,x1)\n",
    "\n",
    "    x = REBNCONV(tf.concat([x,x1],axis=-1), out_ch, 1)\n",
    "\n",
    "    return x + x0\n",
    "\n",
    "def RSU4F(x, mid_ch=12, out_ch=3):\n",
    "\n",
    "    x0 = REBNCONV(x, out_ch, 1)\n",
    "    \n",
    "    x1 = REBNCONV(x0, mid_ch, 1)\n",
    "    x2 = REBNCONV(x1, mid_ch, 2)\n",
    "    x3 = REBNCONV(x2, mid_ch, 4)\n",
    "    \n",
    "    x4 = REBNCONV(x3, mid_ch, 8)\n",
    "    \n",
    "    x = REBNCONV(tf.concat([x4,x3],axis=-1), mid_ch, 4)\n",
    "    x = REBNCONV(tf.concat([x,x2],axis=-1), mid_ch, 2)\n",
    "    x = REBNCONV(tf.concat([x,x1],axis=-1), out_ch, 1)\n",
    "\n",
    "    return x + x0\n",
    "\n",
    "def U2NET(x, out_ch=1):\n",
    "    \n",
    "    x1 = RSU7(x, 32, 64)\n",
    "    x = MaxPool2D(2, 2)(x1)\n",
    "\n",
    "    x2 = RSU6(x, 32, 128)\n",
    "    x = MaxPool2D(2, 2)(x2)\n",
    "\n",
    "    x3 = RSU5(x, 64, 256)\n",
    "    x = MaxPool2D(2, 2)(x3)\n",
    "\n",
    "    x4 = RSU4(x, 128, 512)\n",
    "    x = MaxPool2D(2, 2)(x4)\n",
    "\n",
    "    x5 = RSU4F(x, 256, 512)\n",
    "    x = MaxPool2D(2, 2)(x5)\n",
    "\n",
    "    x6 = RSU4F(x, 256, 512)\n",
    "    x = _upsample_like(x6,x5)\n",
    "\n",
    "    #-----------------decoder--------------------#\n",
    "\n",
    "    x5 = RSU4F(tf.concat([x,x5],axis=-1),256, 512)\n",
    "    x = _upsample_like(x5,x4)\n",
    "\n",
    "    x4 = RSU4(tf.concat([x,x4],axis=-1),128, 256)\n",
    "    x = _upsample_like(x4,x3)\n",
    "\n",
    "    x3 = RSU5(tf.concat([x,x3],axis=-1),64, 128)\n",
    "    x = _upsample_like(x3,x2)\n",
    "\n",
    "    x2 = RSU6(tf.concat([x,x2],axis=-1),32, 64)\n",
    "    x = _upsample_like(x2,x1)\n",
    "    \n",
    "    x1 = RSU7(tf.concat([x,x1],axis=-1),16, 64)\n",
    "\n",
    "    #Side output\n",
    "    x = ZeroPadding2D((1,1))(x1) \n",
    "    d1 = Conv2D(out_ch, 3)(x)\n",
    "    d1 = Activation('sigmoid')(d1)\n",
    "\n",
    "    x = ZeroPadding2D((1,1))(x2) \n",
    "    x = Conv2D(out_ch, 3)(x)\n",
    "    d2 = _upsample_like(x,d1)\n",
    "    d2 = Activation('sigmoid')(d2)\n",
    "    \n",
    "    x = ZeroPadding2D((1,1))(x3) \n",
    "    x = Conv2D(out_ch, 3)(x)\n",
    "    d3 = _upsample_like(x,d1)\n",
    "    d3 = Activation('sigmoid')(d3)\n",
    "    \n",
    "    x = ZeroPadding2D((1,1))(x4) \n",
    "    x = Conv2D(out_ch, 3)(x)\n",
    "    d4 = _upsample_like(x,d1)\n",
    "    d4 = Activation('sigmoid')(d4)\n",
    "    \n",
    "    x = ZeroPadding2D((1,1))(x5) \n",
    "    x = Conv2D(out_ch, 3)(x)\n",
    "    d5 = _upsample_like(x,d1)\n",
    "    d5 = Activation('sigmoid')(d5)\n",
    "    \n",
    "    x = ZeroPadding2D((1,1))(x6) \n",
    "    x = Conv2D(out_ch, 3)(x)\n",
    "    d6 = _upsample_like(x,d1)\n",
    "    d6 = Activation('sigmoid')(d6)\n",
    "\n",
    "    d0 = Conv2D(out_ch, 1)(tf.concat([d1,d2,d3,d4,d5,d6],axis=-1))\n",
    "    d0 = Activation('sigmoid')(d0)\n",
    "\n",
    "    return tf.stack([d0,d1,d2,d3,d4,d5,d6])\n",
    "\n",
    "def U2NETP(x, out_ch=1):\n",
    "    \n",
    "    x1 = RSU7(x, 16, 64)\n",
    "    x = MaxPool2D(2, 2)(x1)\n",
    "\n",
    "    x2 = RSU6(x, 16, 64)\n",
    "    x = MaxPool2D(2, 2)(x2)\n",
    "\n",
    "    x3 = RSU5(x, 16, 64)\n",
    "    x = MaxPool2D(2, 2)(x3)\n",
    "\n",
    "    x4 = RSU4(x, 16, 64)\n",
    "    x = MaxPool2D(2, 2)(x4)\n",
    "\n",
    "    x5 = RSU4F(x, 16, 64)\n",
    "    x = MaxPool2D(2, 2)(x5)\n",
    "\n",
    "    x6 = RSU4F(x, 16, 64)\n",
    "    x = _upsample_like(x6,x5)\n",
    "\n",
    "    #---------------decoder--------------------\n",
    "    x5 = RSU4F(tf.concat([x,x5],axis=-1),16, 64)\n",
    "    x = _upsample_like(x5,x4)\n",
    "\n",
    "    x4 = RSU4(tf.concat([x,x4],axis=-1),16, 64)\n",
    "    x = _upsample_like(x4,x3)\n",
    "\n",
    "    x3 = RSU5(tf.concat([x,x3],axis=-1),16, 64)\n",
    "    x = _upsample_like(x3,x2)\n",
    "\n",
    "    x2 = RSU6(tf.concat([x,x2],axis=-1),16, 64)\n",
    "    x = _upsample_like(x2,x1)\n",
    "    \n",
    "    x1 = RSU7(tf.concat([x,x1],axis=-1),16, 64)\n",
    "\n",
    "    x = ZeroPadding2D((1,1))(x1) \n",
    "    d1 = Conv2D(out_ch, 3)(x)\n",
    "    d1 = Activation('sigmoid')(d1)\n",
    "\n",
    "    x = ZeroPadding2D((1,1))(x2) \n",
    "    x = Conv2D(out_ch, 3)(x)\n",
    "    d2 = _upsample_like(x,d1)\n",
    "    d2 = Activation('sigmoid')(d2)\n",
    "    \n",
    "    x = ZeroPadding2D((1,1))(x3) \n",
    "    x = Conv2D(out_ch, 3)(x)\n",
    "    d3 = _upsample_like(x,d1)\n",
    "    d3 = Activation('sigmoid')(d3)\n",
    "    \n",
    "    x = ZeroPadding2D((1,1))(x4) \n",
    "    x = Conv2D(out_ch, 3)(x)\n",
    "    d4 = _upsample_like(x,d1)\n",
    "    d4 = Activation('sigmoid')(d4)\n",
    "    \n",
    "    x = ZeroPadding2D((1,1))(x5) \n",
    "    x = Conv2D(out_ch, 3)(x)\n",
    "    d5 = _upsample_like(x,d1)\n",
    "    d5 = Activation('sigmoid')(d5)\n",
    "    \n",
    "    x = ZeroPadding2D((1,1))(x6) \n",
    "    x = Conv2D(out_ch, 3)(x)\n",
    "    d6 = _upsample_like(x,d1)\n",
    "    d6 = Activation('sigmoid')(d6)\n",
    "\n",
    "    d0 = Conv2D(out_ch, 1)(tf.concat([d1,d2,d3,d4,d5,d6],axis=-1))\n",
    "    d0 = Activation('sigmoid')(d0)\n",
    "\n",
    "    return tf.stack([d0,d1,d2,d3,d4,d5,d6]) \n",
    "\n",
    "net_input = Input(shape=(256,192,1)) \n",
    "\n",
    "model_output = U2NETP(net_input)\n",
    "\n",
    "model = Model(inputs = net_input, outputs = model_output)\n",
    "\n",
    "lr = 1e-3\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate = lr)\n",
    "\n",
    "bce = BinaryCrossentropy()\n",
    "\n",
    "model.compile(optimizer = opt, loss = loss, metrics = [dice_coef])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tzmqnU2G8R4Q"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oBo2LdeYAZD"
   },
   "source": [
    "**PSPNET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0S3JcQgM3WlI"
   },
   "outputs": [],
   "source": [
    "def conv_block(input_tensor, filters, strides, d_rates):\n",
    "    x = Conv2D(filters[0], kernel_size=1, dilation_rate=d_rates[0])(input_tensor)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters[1], kernel_size=3, strides=strides, padding='same', dilation_rate=d_rates[1])(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters[2], kernel_size=1, dilation_rate=d_rates[2])(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    shortcut = Conv2D(filters[2], kernel_size=1, strides=strides)(input_tensor)\n",
    "    shortcut = BatchNormalization()(shortcut)\n",
    "\n",
    "    x = add([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def identity_block(input_tensor, filters, d_rates):\n",
    "    x = Conv2D(filters[0], kernel_size=1, dilation_rate=d_rates[0])(input_tensor)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters[1], kernel_size=3, padding='same', dilation_rate=d_rates[1])(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters[2], kernel_size=1, dilation_rate=d_rates[2])(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = add([x, input_tensor])\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def pyramid_pooling_block(input_tensor, bin_sizes):\n",
    "    concat_list = [input_tensor]\n",
    "    h = input_tensor.shape[1]\n",
    "    w = input_tensor.shape[2]\n",
    "\n",
    "    for bin_size in bin_sizes:\n",
    "        x = AveragePooling2D(pool_size=(h//bin_size, w//bin_size), strides=(h//bin_size, w//bin_size))(input_tensor)\n",
    "        x = Conv2D(512, kernel_size=1)(x)\n",
    "        x = Lambda(lambda x: tf.image.resize(x, (h, w)))(x)\n",
    "\n",
    "        concat_list.append(x)\n",
    "\n",
    "    return concatenate(concat_list)\n",
    "\n",
    "\n",
    "def pspnet50(num_classes, input_shape):\n",
    "    img_input = Input(input_shape)\n",
    "\n",
    "    x = Conv2D(64, kernel_size=3, strides=(2, 2), padding='same')(img_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(64, kernel_size=3, strides=(1, 1), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(128, kernel_size=3, strides=(1, 1), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding=\"same\")(x)\n",
    "\n",
    "    x = conv_block(x, filters=[64, 64, 256], strides=(1, 1), d_rates=[1, 1, 1])\n",
    "    x = identity_block(x, filters=[64, 64, 256], d_rates=[1, 1, 1])\n",
    "    x = identity_block(x, filters=[64, 64, 256], d_rates=[1, 1, 1])\n",
    "\n",
    "    x = conv_block(x, filters=[128, 128, 512], strides=(2, 2), d_rates=[1, 1, 1])\n",
    "    x = identity_block(x, filters=[128, 128, 512], d_rates=[1, 1, 1])\n",
    "    x = identity_block(x, filters=[128, 128, 512], d_rates=[1, 1, 1])\n",
    "    x = identity_block(x, filters=[128, 128, 512], d_rates=[1, 1, 1])\n",
    "\n",
    "    x = conv_block(x, filters=[256, 256, 1024], strides=(1, 1), d_rates=[1, 2, 1])\n",
    "    x = identity_block(x, filters=[256, 256, 1024], d_rates=[1, 2, 1])\n",
    "    x = identity_block(x, filters=[256, 256, 1024], d_rates=[1, 2, 1])\n",
    "    x = identity_block(x, filters=[256, 256, 1024], d_rates=[1, 2, 1])\n",
    "    x = identity_block(x, filters=[256, 256, 1024], d_rates=[1, 2, 1])\n",
    "    x = identity_block(x, filters=[256, 256, 1024], d_rates=[1, 2, 1])\n",
    "\n",
    "    x = conv_block(x, filters=[512, 512, 2048], strides=(1, 1), d_rates=[1, 4, 1])\n",
    "    x = identity_block(x, filters=[512, 512, 2048], d_rates=[1, 4, 1])\n",
    "    x = identity_block(x, filters=[512, 512, 2048], d_rates=[1, 4, 1])\n",
    "\n",
    "    x = pyramid_pooling_block(x, [1, 2, 3, 6])\n",
    "\n",
    "    x = Conv2D(512, kernel_size=3, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "\n",
    "    x = Conv2D(num_classes, kernel_size=1)(x)\n",
    "    x = Conv2DTranspose(num_classes, kernel_size=(16, 16), strides=(8, 8), padding='same')(x)\n",
    "    x = Activation('sigmoid')(x)\n",
    "\n",
    "    model = Model(img_input, x)\n",
    "    model.compile(optimizer=Adam(),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=[dice_coef])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EympstHsYCga"
   },
   "source": [
    "**UNET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ncVE6pOBOa41"
   },
   "outputs": [],
   "source": [
    "def unet(num_classes, input_shape, vgg_weight_path=None):\n",
    "    img_input = Input(input_shape)\n",
    "\n",
    "    # Block 1\n",
    "    x = Conv2D(64, (3, 3), padding='same', name='block1_conv1')(img_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(64, (3, 3), padding='same', name='block1_conv2')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    block_1_out = Activation('relu')(x)\n",
    "\n",
    "    x = MaxPooling2D()(block_1_out)\n",
    "\n",
    "    # Block 2\n",
    "    x = Conv2D(128, (3, 3), padding='same', name='block2_conv1')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(128, (3, 3), padding='same', name='block2_conv2')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    block_2_out = Activation('relu')(x)\n",
    "\n",
    "    x = MaxPooling2D()(block_2_out)\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(256, (3, 3), padding='same', name='block3_conv1')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(256, (3, 3), padding='same', name='block3_conv2')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(256, (3, 3), padding='same', name='block3_conv3')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    block_3_out = Activation('relu')(x)\n",
    "\n",
    "    x = MaxPooling2D()(block_3_out)\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(512, (3, 3), padding='same', name='block4_conv1')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(512, (3, 3), padding='same', name='block4_conv2')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(512, (3, 3), padding='same', name='block4_conv3')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    block_4_out = Activation('relu')(x)\n",
    "\n",
    "    x = MaxPooling2D()(block_4_out)\n",
    "\n",
    "    # Block 5\n",
    "    x = Conv2D(512, (3, 3), padding='same', name='block5_conv1')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(512, (3, 3), padding='same', name='block5_conv2')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(512, (3, 3), padding='same', name='block5_conv3')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    for_pretrained_weight = MaxPooling2D()(x)\n",
    "\n",
    "    # Load pretrained weights.\n",
    "    if vgg_weight_path is not None:\n",
    "        vgg16 = Model(img_input, for_pretrained_weight)\n",
    "        vgg16.load_weights(vgg_weight_path, by_name=True)\n",
    "\n",
    "    # UP 1\n",
    "    x = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = concatenate([x, block_4_out])\n",
    "    x = Conv2D(512, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(512, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # UP 2\n",
    "    x = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = concatenate([x, block_3_out])\n",
    "    x = Conv2D(256, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(256, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # UP 3\n",
    "    x = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = concatenate([x, block_2_out])\n",
    "    x = Conv2D(128, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(128, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # UP 4\n",
    "    x = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = concatenate([x, block_1_out])\n",
    "    x = Conv2D(64, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(64, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # last conv\n",
    "    x = Conv2D(num_classes, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    model = Model(img_input, x)\n",
    "    model.compile(optimizer=Adam(),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=[dice_coef])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRokptoDx9ny"
   },
   "outputs": [],
   "source": [
    "model = unet(1,(320,240,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-u2YVJ9i4nef",
    "outputId": "cec98478-09bf-4a7c-873f-edacc16796e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting split-folders\n",
      "  Downloading https://files.pythonhosted.org/packages/b8/5f/3c2b2f7ea5e047c8cdc3bb00ae582c5438fcdbbedcc23b3cc1c2c7aae642/split_folders-0.4.3-py3-none-any.whl\n",
      "Installing collected packages: split-folders\n",
      "Successfully installed split-folders-0.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install split-folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h8M04Kn74j-C",
    "outputId": "e2ef7943-88ff-4009-af43-7878569d3e1a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 621 files [00:00, 5304.76 files/s]\n",
      "Copying files: 621 files [00:00, 8911.81 files/s]\n"
     ]
    }
   ],
   "source": [
    "import splitfolders  # or import split_folders\n",
    "splitfolders.ratio(\"unet_data_edges//images\", output=\"output//images\", seed=1337, ratio=(.8, .2), group_prefix=None) # default values\n",
    "splitfolders.ratio(\"unet_data_edges//mask\", output=\"output//mask\", seed=1337, ratio=(.8, .2), group_prefix=None) # default values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CoextCcPYFpX"
   },
   "source": [
    "**Image Augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gc77lzYnuvRI"
   },
   "outputs": [],
   "source": [
    "def gen(path): \n",
    "    SEED = 100\n",
    "    image_data_generator = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        width_shift_range = 0.05,\n",
    "        height_shift_range = 0.05,\n",
    "        shear_range = 0.1,\n",
    "        horizontal_flip = True,\n",
    "        vertical_flip = True,\n",
    "        rotation_range = 5,\n",
    "        zoom_range = 0.1,\n",
    "        fill_mode=\"constant\",\n",
    "        cval=0,\n",
    "    ).flow_from_directory('output/'+'images/'+path, color_mode = 'grayscale', class_mode=None, batch_size = 4, target_size = (256, 192),interpolation = 'nearest', seed = SEED)\n",
    "\n",
    "    mask_data_generator = ImageDataGenerator(\n",
    "        \n",
    "        width_shift_range = 0.05,\n",
    "        height_shift_range = 0.05,\n",
    "        shear_range = 0.1,\n",
    "        horizontal_flip = True,\n",
    "        vertical_flip = True,\n",
    "        rotation_range = 5,\n",
    "        zoom_range = 0.1,\n",
    "        fill_mode=\"constant\",\n",
    "        cval=0,\n",
    "        preprocessing_function = lambda x: np.where(x>0, 1, 0).astype(x.dtype)\n",
    "    ).flow_from_directory('output/'+'mask/'+path, color_mode = 'grayscale', class_mode=None, batch_size = 4, target_size = (256, 192), interpolation = 'nearest', seed = SEED)\n",
    "\n",
    "    return image_data_generator,mask_data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_YF6i375u0gQ",
    "outputId": "32f023e3-6ec2-439b-a0c7-6b588d57b755"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "124/124 [==============================] - 21s 172ms/step - loss: 0.4680 - dice_coef: 0.6071 - val_loss: 0.4952 - val_dice_coef: 0.5982\n",
      "Epoch 2/30\n",
      "124/124 [==============================] - 21s 172ms/step - loss: 0.4670 - dice_coef: 0.6063 - val_loss: 0.4830 - val_dice_coef: 0.6006\n",
      "Epoch 3/30\n",
      "124/124 [==============================] - 21s 172ms/step - loss: 0.4683 - dice_coef: 0.6093 - val_loss: 0.4839 - val_dice_coef: 0.5975\n",
      "Epoch 4/30\n",
      "124/124 [==============================] - 21s 171ms/step - loss: 0.4727 - dice_coef: 0.6065 - val_loss: 0.4894 - val_dice_coef: 0.6044\n",
      "Epoch 5/30\n",
      "124/124 [==============================] - 21s 172ms/step - loss: 0.4679 - dice_coef: 0.6091 - val_loss: 0.4788 - val_dice_coef: 0.6006\n",
      "Epoch 6/30\n",
      "124/124 [==============================] - 21s 171ms/step - loss: 0.4659 - dice_coef: 0.6080 - val_loss: 0.5003 - val_dice_coef: 0.6003\n",
      "Epoch 7/30\n",
      "124/124 [==============================] - 21s 172ms/step - loss: 0.4688 - dice_coef: 0.6070 - val_loss: 0.4990 - val_dice_coef: 0.6018\n",
      "Epoch 8/30\n",
      "124/124 [==============================] - 21s 171ms/step - loss: 0.4723 - dice_coef: 0.6096 - val_loss: 0.4817 - val_dice_coef: 0.6017\n",
      "Epoch 9/30\n",
      "124/124 [==============================] - 21s 171ms/step - loss: 0.4679 - dice_coef: 0.6109 - val_loss: 0.4971 - val_dice_coef: 0.6026\n",
      "Epoch 10/30\n",
      "124/124 [==============================] - 21s 171ms/step - loss: 0.4697 - dice_coef: 0.6114 - val_loss: 0.4865 - val_dice_coef: 0.6027\n",
      "Epoch 11/30\n",
      "124/124 [==============================] - 21s 171ms/step - loss: 0.4672 - dice_coef: 0.6114 - val_loss: 0.4873 - val_dice_coef: 0.6048\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 12/30\n",
      "124/124 [==============================] - 21s 171ms/step - loss: 0.4692 - dice_coef: 0.6114 - val_loss: 0.4882 - val_dice_coef: 0.6019\n",
      "Epoch 13/30\n",
      "124/124 [==============================] - 21s 171ms/step - loss: 0.4682 - dice_coef: 0.6112 - val_loss: 0.5003 - val_dice_coef: 0.6026\n",
      "Epoch 14/30\n",
      "124/124 [==============================] - 21s 171ms/step - loss: 0.4677 - dice_coef: 0.6107 - val_loss: 0.4851 - val_dice_coef: 0.5991\n",
      "Epoch 15/30\n",
      "124/124 [==============================] - 21s 171ms/step - loss: 0.4670 - dice_coef: 0.6103 - val_loss: 0.5012 - val_dice_coef: 0.6021\n",
      "Epoch 16/30\n",
      "124/124 [==============================] - 21s 171ms/step - loss: 0.4654 - dice_coef: 0.6126 - val_loss: 0.4787 - val_dice_coef: 0.6050\n",
      "Epoch 17/30\n",
      "124/124 [==============================] - 21s 172ms/step - loss: 0.4658 - dice_coef: 0.6105 - val_loss: 0.4908 - val_dice_coef: 0.6069\n",
      "Epoch 18/30\n",
      "124/124 [==============================] - 21s 171ms/step - loss: 0.4664 - dice_coef: 0.6111 - val_loss: 0.4902 - val_dice_coef: 0.6014\n",
      "Epoch 19/30\n",
      "124/124 [==============================] - 21s 171ms/step - loss: 0.4659 - dice_coef: 0.6116 - val_loss: 0.4924 - val_dice_coef: 0.6065\n",
      "Epoch 20/30\n",
      "124/124 [==============================] - 21s 171ms/step - loss: 0.4661 - dice_coef: 0.6095 - val_loss: 0.4845 - val_dice_coef: 0.6060\n",
      "Epoch 21/30\n",
      "124/124 [==============================] - 21s 171ms/step - loss: 0.4678 - dice_coef: 0.6104 - val_loss: 0.4885 - val_dice_coef: 0.6018\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "Epoch 22/30\n",
      "124/124 [==============================] - 21s 171ms/step - loss: 0.4690 - dice_coef: 0.6119 - val_loss: 0.4969 - val_dice_coef: 0.6026\n",
      "Epoch 23/30\n",
      "124/124 [==============================] - 21s 171ms/step - loss: 0.4623 - dice_coef: 0.6092 - val_loss: 0.4847 - val_dice_coef: 0.6034\n",
      "Epoch 24/30\n",
      "124/124 [==============================] - 21s 171ms/step - loss: 0.4630 - dice_coef: 0.6094 - val_loss: 0.4889 - val_dice_coef: 0.6052\n",
      "Epoch 25/30\n",
      "124/124 [==============================] - 21s 171ms/step - loss: 0.4676 - dice_coef: 0.6122 - val_loss: 0.4847 - val_dice_coef: 0.6039\n",
      "Epoch 26/30\n",
      "124/124 [==============================] - 21s 171ms/step - loss: 0.4693 - dice_coef: 0.6113 - val_loss: 0.4836 - val_dice_coef: 0.6002\n",
      "Epoch 27/30\n",
      "124/124 [==============================] - 21s 171ms/step - loss: 0.4657 - dice_coef: 0.6107 - val_loss: 0.4873 - val_dice_coef: 0.6037\n",
      "Epoch 28/30\n",
      "124/124 [==============================] - 21s 171ms/step - loss: 0.4629 - dice_coef: 0.6096 - val_loss: 0.4879 - val_dice_coef: 0.6030\n",
      "Epoch 29/30\n",
      "124/124 [==============================] - 21s 171ms/step - loss: 0.4656 - dice_coef: 0.6109 - val_loss: 0.4911 - val_dice_coef: 0.6064\n",
      "Epoch 30/30\n",
      "124/124 [==============================] - 21s 171ms/step - loss: 0.4709 - dice_coef: 0.6104 - val_loss: 0.4806 - val_dice_coef: 0.6049\n"
     ]
    }
   ],
   "source": [
    "# from keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau\n",
    "# tr_img,tr_mask = gen('train')\n",
    "# t_gen = zip(tr_img,tr_mask)\n",
    "# vl_img,vl_mask = gen('val')\n",
    "# vl_gen = zip(vl_img,vl_mask)\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'modelunet.h5', \n",
    "    monitor='val_loss', \n",
    "    verbose=0, \n",
    "    save_best_only=True, \n",
    "    save_weights_only=False,\n",
    "    mode='auto'\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    min_lr=1e-6,\n",
    "    min_delta=0.05\n",
    ")\n",
    "# model.compile(optimizer=Adam(),\n",
    "#                   loss='binary_crossentropy',\n",
    "#                   metrics=[dice_coef])\n",
    "# model.compile(optimizer = opt, loss = loss, metrics = [dice_coef])\n",
    "\n",
    "STEP_SIZE_TRAIN=tr_img.n//tr_img.batch_size\n",
    "STEP_SIZE_VALID=vl_img.n//vl_img.batch_size\n",
    "history = model.fit_generator(t_gen,steps_per_epoch=STEP_SIZE_TRAIN,validation_data=vl_gen,validation_steps=STEP_SIZE_VALID,epochs=30, callbacks=[ reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ZpiUbfHqDTD"
   },
   "outputs": [],
   "source": [
    "model.save(\"u2net_v3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fLqovkYMurrf"
   },
   "outputs": [],
   "source": [
    "model.save_weights('u2net_edge.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F9CJCSn_spwy"
   },
   "outputs": [],
   "source": [
    "  image_test_generator = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "    ).flow_from_directory('input', color_mode = 'grayscale',target_size = (400, 304))\n",
    "\n",
    "predict = model.predict_generator(image_test_generator)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
